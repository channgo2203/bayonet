\hypertarget{classbayonet_1_1_gibbs_sampler}{\section{bayonet\-:\-:Gibbs\-Sampler Class Reference}
\label{classbayonet_1_1_gibbs_sampler}\index{bayonet\-::\-Gibbs\-Sampler@{bayonet\-::\-Gibbs\-Sampler}}
}


Implementation of the Gibbs sampler.  




{\ttfamily \#include $<$Gibbs\-Sampler.\-h$>$}

\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
std\-::vector$<$ unsigned int $>$ \hyperlink{classbayonet_1_1_gibbs_sampler_ac789fc64fd349769633ab84ea5e59f70}{Return\-Sample} (\hyperlink{classbayonet_1_1_bayesnet}{bayonet\-::\-Bayesnet} \&net)
\item 
std\-::vector$<$ unsigned int $>$ \hyperlink{classbayonet_1_1_gibbs_sampler_aa3e7d24f8eae4ee4c4ed0a2c16a9b5fa}{Return\-Sample} (\hyperlink{classbayonet_1_1_bayesnet}{bayonet\-::\-Bayesnet} \&net, std\-::vector$<$ unsigned int $>$ starting\-Vector)
\item 
std\-::vector$<$ std\-::vector\\*
$<$ unsigned int $>$ $>$ \hyperlink{classbayonet_1_1_gibbs_sampler_a625b16c10dc1d4fea3899fd5a0b3c921}{Accumulate\-Samples} (\hyperlink{classbayonet_1_1_bayesnet}{Bayesnet} \&net, unsigned int cycles)
\item 
void \hyperlink{classbayonet_1_1_gibbs_sampler_af375d044a2e0c45a384f37ee4761a2ba}{Print\-Sample} (\hyperlink{classbayonet_1_1_bayesnet}{bayonet\-::\-Bayesnet} \&net, unsigned int cycles=1)
\item 
\hyperlink{classbayonet_1_1_joint_probability_table}{Joint\-Probability\-Table} \hyperlink{classbayonet_1_1_gibbs_sampler_a29dfcfda3462de83fa89672768d690f4}{Return\-Joint\-Probability\-Table} (\hyperlink{classbayonet_1_1_bayesnet}{bayonet\-::\-Bayesnet} \&net, unsigned int cycles)
\end{DoxyCompactItemize}


\subsection{Detailed Description}
Implementation of the Gibbs sampler. 

The Gibbs sampler is part of the Markov Chain Monte Carlo (M\-C\-M\-C) methods. These family of methods work differently from rejection and L\-W sampling. Each sample is generated by making a random change to the preceding one. The algorithm starts with a random state where only the evidence variables are fixed at their observed values. After the initialization, the sampling is done conditioned on the current values of the variables in the Markov blanket of the node. The algorithm then continue moving at random, flipping one variable at a time but leaving the evidences unchanged. 

\subsection{Member Function Documentation}
\hypertarget{classbayonet_1_1_gibbs_sampler_a625b16c10dc1d4fea3899fd5a0b3c921}{\index{bayonet\-::\-Gibbs\-Sampler@{bayonet\-::\-Gibbs\-Sampler}!Accumulate\-Samples@{Accumulate\-Samples}}
\index{Accumulate\-Samples@{Accumulate\-Samples}!bayonet::GibbsSampler@{bayonet\-::\-Gibbs\-Sampler}}
\subsubsection[{Accumulate\-Samples}]{\setlength{\rightskip}{0pt plus 5cm}std\-::vector$<$ std\-::vector$<$ unsigned int $>$ $>$ bayonet\-::\-Gibbs\-Sampler\-::\-Accumulate\-Samples (
\begin{DoxyParamCaption}
\item[{{\bf Bayesnet} \&}]{net, }
\item[{unsigned int}]{cycles}
\end{DoxyParamCaption}
)}}\label{classbayonet_1_1_gibbs_sampler_a625b16c10dc1d4fea3899fd5a0b3c921}
This method is different from the same methods in the other samplers. The first sample is obtained at random. The next samples are choosen for each node picking up a value from the Markov blanket of the node. This probability is proportional to the probability of the variable given its parents times the probability of each child given its respective parents.


\begin{DoxyParams}{Parameters}
{\em net} & the Bayesian network to use for picking up the sample. \\
\hline
{\em cycles} & the number of iterations \\
\hline
\end{DoxyParams}
\hypertarget{classbayonet_1_1_gibbs_sampler_af375d044a2e0c45a384f37ee4761a2ba}{\index{bayonet\-::\-Gibbs\-Sampler@{bayonet\-::\-Gibbs\-Sampler}!Print\-Sample@{Print\-Sample}}
\index{Print\-Sample@{Print\-Sample}!bayonet::GibbsSampler@{bayonet\-::\-Gibbs\-Sampler}}
\subsubsection[{Print\-Sample}]{\setlength{\rightskip}{0pt plus 5cm}void bayonet\-::\-Gibbs\-Sampler\-::\-Print\-Sample (
\begin{DoxyParamCaption}
\item[{{\bf bayonet\-::\-Bayesnet} \&}]{net, }
\item[{unsigned int}]{cycles = {\ttfamily 1}}
\end{DoxyParamCaption}
)}}\label{classbayonet_1_1_gibbs_sampler_af375d044a2e0c45a384f37ee4761a2ba}
It prints the result of the sampling. It is possible to do it for different iterations.


\begin{DoxyParams}{Parameters}
{\em net} & the Bayesian network to use for picking up the sample. \\
\hline
{\em cycles} & the number of iterations \\
\hline
\end{DoxyParams}
\hypertarget{classbayonet_1_1_gibbs_sampler_a29dfcfda3462de83fa89672768d690f4}{\index{bayonet\-::\-Gibbs\-Sampler@{bayonet\-::\-Gibbs\-Sampler}!Return\-Joint\-Probability\-Table@{Return\-Joint\-Probability\-Table}}
\index{Return\-Joint\-Probability\-Table@{Return\-Joint\-Probability\-Table}!bayonet::GibbsSampler@{bayonet\-::\-Gibbs\-Sampler}}
\subsubsection[{Return\-Joint\-Probability\-Table}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Joint\-Probability\-Table} bayonet\-::\-Gibbs\-Sampler\-::\-Return\-Joint\-Probability\-Table (
\begin{DoxyParamCaption}
\item[{{\bf bayonet\-::\-Bayesnet} \&}]{net, }
\item[{unsigned int}]{cycles}
\end{DoxyParamCaption}
)}}\label{classbayonet_1_1_gibbs_sampler_a29dfcfda3462de83fa89672768d690f4}
It creates a Joint Probability table starting from the Bayesian network and sampling for the number of iterations specified.


\begin{DoxyParams}{Parameters}
{\em net} & the Bayesian network to use for picking up the sample. \\
\hline
{\em cycles} & the number of iterations \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
it return a Joint Probability Table object 
\end{DoxyReturn}
\hypertarget{classbayonet_1_1_gibbs_sampler_ac789fc64fd349769633ab84ea5e59f70}{\index{bayonet\-::\-Gibbs\-Sampler@{bayonet\-::\-Gibbs\-Sampler}!Return\-Sample@{Return\-Sample}}
\index{Return\-Sample@{Return\-Sample}!bayonet::GibbsSampler@{bayonet\-::\-Gibbs\-Sampler}}
\subsubsection[{Return\-Sample}]{\setlength{\rightskip}{0pt plus 5cm}std\-::vector$<$ unsigned int $>$ bayonet\-::\-Gibbs\-Sampler\-::\-Return\-Sample (
\begin{DoxyParamCaption}
\item[{{\bf bayonet\-::\-Bayesnet} \&}]{net}
\end{DoxyParamCaption}
)}}\label{classbayonet_1_1_gibbs_sampler_ac789fc64fd349769633ab84ea5e59f70}
It returns a single sample picking up it from the Bayesian network


\begin{DoxyParams}{Parameters}
{\em net} & the Bayesian network to use for picking up the sample. \\
\hline
\end{DoxyParams}
\hypertarget{classbayonet_1_1_gibbs_sampler_aa3e7d24f8eae4ee4c4ed0a2c16a9b5fa}{\index{bayonet\-::\-Gibbs\-Sampler@{bayonet\-::\-Gibbs\-Sampler}!Return\-Sample@{Return\-Sample}}
\index{Return\-Sample@{Return\-Sample}!bayonet::GibbsSampler@{bayonet\-::\-Gibbs\-Sampler}}
\subsubsection[{Return\-Sample}]{\setlength{\rightskip}{0pt plus 5cm}std\-::vector$<$ unsigned int $>$ bayonet\-::\-Gibbs\-Sampler\-::\-Return\-Sample (
\begin{DoxyParamCaption}
\item[{{\bf bayonet\-::\-Bayesnet} \&}]{net, }
\item[{std\-::vector$<$ unsigned int $>$}]{starting\-Vector}
\end{DoxyParamCaption}
)}}\label{classbayonet_1_1_gibbs_sampler_aa3e7d24f8eae4ee4c4ed0a2c16a9b5fa}
It returns a single sample picking up it from the Bayesian network


\begin{DoxyParams}{Parameters}
{\em net} & the Bayesian network to use for picking up the sample. \\
\hline
{\em starting\-Vector} & it is the vector with the current states of all the nodes it is used as starting point for the Markov chain sampler \\
\hline
\end{DoxyParams}


The documentation for this class was generated from the following files\-:\begin{DoxyCompactItemize}
\item 
include/Gibbs\-Sampler.\-h\item 
src/Gibbs\-Sampler.\-cpp\end{DoxyCompactItemize}
